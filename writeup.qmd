---
title: Final Project Write-Up
subtitle: INFO 523
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Introduction

~\*Note for peer review & Dr. Chen that the proposal template I followed did not include "asking a question". So, that was not part of my project goal.~

This project develops a book recommendation system using a hybrid approach by combining content-based similarity from book metadata with collaborative filtering based on user rating patterns. Using TF-IDF text features and a KNN-based model from the Surprise library, the system generates personalized reading suggestions that overcome the limitations of using either model alone.

This write-up explains the decisions made to improve data quality, interpretability, and model performance.

## Datasets

The datasets used in this project were downloaded from [Kaggle.](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data)

#### Books.csv

`ISBN` - unique identifier of each book

`Book-Title` - title of the book

`Book-Author` - author of the book

`Year-Of-Publication` - year the book was published

`Publisher` - company that published the book

`Image-URL-S` - URL to a small cover image

`Image-URL-M` - URL to a medium cover image

`Image-URL-L` - URL to a large cover image

#### Ratings.csv:

`User-ID - anonymous` unique id for each user

`ISBN` - unique identifier for each book

`Book-Rating` - rating given to book from User_ID

## Approach and Mining Methods

### Data Cleaning

-   Standardized column names for both datasets so that they were compatible with Pandas and a clean schema makes a pipeline reproducible.

<!-- -->

-   For `books.csv`, I removed invalid publication years, rows with missing values for `Book_Author` and `Publisher`, any books published by Scholastic (kids books), and known foreign-language publishers. I was not able to interpret the quality of recommendations when all the book titles were in German.

-   Normalized text fields in `books.csv` which improves TF-IDF tokenization and avoids reading "harry potter" as a different book than "Harry Potter".

-   For `ratings.csv` , I removed any ratings set at 0. These were instances where users clicked or viewed the book but not actually leave a rating. Keeping these rows would have distorted the algorithm.

### Exploratory Data Analysis (EDA)

-   Found outliers in `Year_Of_Publication`. Removed any books published before 1950 so all books were mostly relevant.

-   Checked for other outliers by looked at the distribution of ratings, ratings per user, and ratings per books. Determined that any extreme values were either popular books or avid readers.

-   Removed ISBN's that existed within `ratings.csv` but not `books.csv`. Need the book metadata for the model.

-   Merged the two datasets using `ISBN`. Removed any duplicates. Filtered out any users or books with less than 5 ratings because CF models cannot learn meaningful user-item relationships with sparse behavior.

### Modeling Decisions

-   Created a user-item rating matrix for collaborative filtering, a TF-IDF matrix for content-based filtering, and an indexed book catalog for content-based similarity. This was all done to either reduce noise and prevent memory explosion (VSCode likes to crash) or to improve model stability.

-   For the content-based filter, used TF-IDF over combined text fields (`Book_Title`, `Book_Author`, and `Publisher`). Since genre was not provided, I used the publisher as most companies produce books in the same realm of genre and/or with the same target demographic in mind.

-   For the collaborative filtering, used Surprise's item-item cosine similarity to add personalization in suggestions that is based on how readers rate books. Amother all the great reasons the Surprise library is normally used, it was specifically used in this project because it creates internal ID Mapping so there is no manual encoding needed and the library makes it easy to extract CF similarity vectors and blend them with content-based scores.

-   The hybrid model computes a similarity score from both the CBF and CF, then blends those scores using a parameter (alpha), which controls the balance between the two. I chose alpha = 0.6 so the content features have slightly more weight than rating similarities.

    $$
    \text{HybridSim}(i, j)
    = \alpha \cdot \text{ContentSim}(i, j)
    + 1 - \alpha) \cdot \text{CFSim}(i, j)
    $$

-   If a user rated a book highly (8 -10), that encouraged recommendations with similarity patterns. If a user rated a book poorly (1-3), the model would weaken similarity's effect or avoid recommending books with strong overlap.

### Testing and Evaluation

-   Created a function that prints the book that a user read along with the rating given, and the top 10 recommendations based on that. The hybrid score range was around 0.30-0.60, with most scores are in the 0.4 range. For reference a score of 0.70 would be a near duplicate book and a score of 0.15 would be an unrelated book. I thought the recommendations were mostly accurate.

-   There are a few limitations. The hybrid model ensures that the ISBN just rated is not included in its own recommendation list, however books can have different ISBN's if published by different companies. The book title was also slightly different in this situation. I also saw the same book recommended if there was a row for the hardcover and paperback version (different ISBNs and different titles).

-   The model could be improved by adding book genres and sub-genres. Also by incorporating user-level demographic patterns like age and location. Most especially, I want the model to take into consideration book descriptions by using BERT-based embeddings for better recommendations.

## Code

The full code is the `allwork.ipynb` file under the `work` folder.

~AI\ Disclosure:\ AI\ was\ used\ to\ narrow\ down\ a\ list\ of\ 709\ items\ that\ was\ eventually\ removed\ from\ the\ project.\ No\ AI\ was\ used\ in\ the\ final\ code.~

<details>

<summary><strong>Click to view full code</strong></summary>

```{python}
#| eval: false
#| echo: true

# # The system

# %%
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Dataset, Reader, KNNBasic


# %% [markdown]
# ## Part 1: Data Exploration and Preprocessing
# ### Including: Data Retrieval, Exploratory Data Analysis (EDA), Data Cleaning, & Feature Engineering

# %%
books = pd.read_csv("../data/user_data/Books.csv", low_memory=False)
ratings = pd.read_csv("../data/user_data/Ratings.csv")

# %%
books.head()

# %%
books.info()

# %%
ratings.head()

# %%
ratings.info()

# %%
print("Books:", books.shape)
print("Ratings:", ratings.shape)

# %% [markdown]
# #### Clean the Books.csv dataset

# %%
#Fix column names

books.columns = books.columns.str.replace("-","_").str.replace(" ", "_")

# %%
#Remove invalid publication years

before = books.shape[0]

books['Year_Of_Publication'] = pd.to_numeric(books['Year_Of_Publication'], errors='coerce')
books = books[(books['Year_Of_Publication'] >=1800) & (books['Year_Of_Publication']<= 2025)]

after = books.shape[0]

print(f"Rows removed: {before-after}")

# %%
#Normalize book author and book titles

books['Book_Author'] = books['Book_Author'].str.strip().str.title()
books['Book_Title'] = books['Book_Title'].str.strip().str.title()

# %%
books.head()

# %%
books.info()

# %%
books[books['Book_Author'].isna()]


# %%
#Dropping those two rows because they do not matter

books = books.dropna(subset=["Book_Author"])

# %%
books[books['Publisher'].isna()]

# %%
#Also dropping those two rows because they do not matter

books = books.dropna(subset=["Publisher"])

# %%
#Removing books published by known foreign-language publishers
#Produce books in langauges I do not understand, so excluding them ensures the results remain interpretable

before = books.shape[0]

foreign_publishers = [
    "Goldmann",
    "Heyne",
    "Gallimard",
    "LÃ?Â¼bbe",
    "Distribooks",
    "Eichborn",
    "Diogenes Verlag",
    "Carlsen Verlag GmbH"
]

books = books[
    ~books['Publisher'].str.contains("|".join(foreign_publishers),case = False, na = False)
]

after = books.shape[0]

print(f"Rows removed: {before-after}")

# %%
#Removing Scholastic publisher since that is kids books

books = books[
    ~books['Publisher'].str.contains("Scholastic", case=False, na=False)]

# %%
books.info()

#Im happy with that, let's move on

# %% [markdown]
# #### Cleaning the Ratings.csv dataset

# %%
#Looking at the distribution of ratings

ratings['Book-Rating'].value_counts().sort_index()

# %%
#Dropping rows with rating = 0
#Those are instances where users clicked or viewed on the book but did not give a rating.
#Keeping the zeros will distort the algorithm 

#The dataset will still have ~400k+ ratings amount ~100k users that cover ~100k books

ratings['Book-Rating'] = pd.to_numeric(ratings['Book-Rating'], errors='coerce')
ratings = ratings[ratings['Book-Rating'] >0]

# %%
ratings['Book-Rating'].value_counts().sort_index()

# %%
ratings.info()

# %%
#Fix column names

ratings.columns = ratings.columns.str.replace("-","_").str.replace(" ", "_")

# %%
ratings.head()

# %% [markdown]
# ### Moving on to EDA

# %%
print("Books:", books.shape)
print("Ratings:", ratings.shape)

# %%
#Checking Publication Date for Outliers

books['Year_Of_Publication'].hist(bins=40, figsize=(8,4))

# %%
#As expected, there are outliers.
#Getting rid of rows with books published before 1950 for relevancy

books['Year_Of_Publication']= books['Year_Of_Publication'].clip(lower=1950,upper=2024)

# %%
books['Year_Of_Publication'].hist(bins=40, figsize=(8,4))

#Looks much better

# %%
#Top 10 authors

books['Book_Author'].value_counts().head(10)

# %%
#Top 10 Publishers

books['Publisher'].value_counts().head(10)

# %%
#Plot for distribution of Ratings

rating_counts = ratings['Book_Rating'].value_counts().sort_index()

plt.figure(figsize=(8,4))
sns.barplot(x=rating_counts.index, y=rating_counts.values)
plt.xlabel("Rating")
plt.ylabel("Count")
plt.title("Distribution of Ratings")
plt.show()

# %%
#Number of ratings per user

ratings['User_ID'].value_counts().describe()

#This is highly skewed

# %%
#I am worried the users that have contributed thousands of rankings will give bias to the model

ratings['User_ID'].value_counts().head(10)

#However, the system will be able to give better matches to users with more ratings
#Keeping them for now

# %%
#Ratings per book

ratings['ISBN'].value_counts().describe()

#Outliers exist but it is not an error
#More popular books will have more rating

# %%
#Checking that all ISBNs in the ratings data also exists in the books data

missing_books = ~ratings['ISBN'].isin(books['ISBN'])
print(f"Books rated that are not in Books Data:", missing_books.sum())

#This a problem for when I merge the data, as the ISBNs not in the books data will not have any metadata

# %%
#Removing the ratings with ISBN's that are not in the books data

ratings_clean = ratings[ratings['ISBN'].isin(books['ISBN'])]

# %%
#Now books and ratings data are ready to merge

ratings_merged = ratings_clean.merge(
    books,
    on='ISBN',
    how='inner'
)

ratings_merged.head()

# %%
print("Ratings Merged:", ratings_merged.shape)

ratings_merged.info()

# %%
#Removing duplicate ratings as a sanity check

ratings_merged.drop_duplicates(['User_ID', 'ISBN'], keep='first', inplace=True)

ratings_merged.info()

#There were none

# %%
#Filter books with enough ratings
#The CF KNN model will perform better

book_counts = ratings_merged['ISBN'].value_counts()
valid_books = book_counts[book_counts >= 5].index

ratings_filt = ratings_merged[ratings_merged['ISBN'].isin(valid_books)]

# %%
#Filter users with enough ratings
#Again, the CF KNN model will perform better

user_counts = ratings_filt['User_ID'].value_counts()
valid_users = user_counts[user_counts >= 5].index

ratings_filt= ratings_filt[ratings_filt['User_ID'].isin(valid_users)]

# %% [markdown]
# #### Create Modeling Matrices

# %%
#User-Item Rating Matrix
#For Collaborative Filtering

user_ids_f = ratings_filt['User_ID'].astype('category').cat.codes
book_ids_f = ratings_filt['ISBN'].astype('category').cat.codes
ratings_vals_f = ratings_filt['Book_Rating'].astype(float).values

user_item_sparse_f = csr_matrix(
    (ratings_vals_f, (user_ids_f, book_ids_f))
)

user_item_sparse_f

# %%
#Checking sparsity

print("shape:", user_item_sparse_f.shape)
print("nonzero entries:", user_item_sparse_f.count_nonzero())
print("sparsity:", 1 - (user_item_sparse_f.count_nonzero()/ np.prod(user_item_sparse_f.shape)))

# %%
#Build an indexed version of books

isbn_categories = ratings_filt['ISBN'].unique()

books_indexed = books.drop_duplicates('ISBN').set_index('ISBN')

books_indexed.head()

# %%
#Select only books used in ratings
#Return ISBN as a column

books_for_model = books_indexed.loc[books_indexed.index.isin(isbn_categories)].reset_index().rename(columns={'index':'ISBN'})

books_for_model['Book_Title'] = books_for_model['Book_Title'].fillna('')
books_for_model['Book_Author'] = books_for_model['Book_Author'].fillna('')
books_for_model['Publisher'] = books_for_model['Publisher'].fillna('')

books_for_model['text'] =(
    books_for_model['Book_Title'].fillna('') +''+
    books_for_model['Book_Author'].fillna('') +''+
    books_for_model['Publisher'].fillna('')
).str.lower()

books_for_model[['ISBN', 'text']].head()

# %%
#Build TF-IDF Matrix
#Limiting vocab size to prevent memory explosion

tfidf = TfidfVectorizer(
    max_features=50000,
    stop_words='english',
    ngram_range=(1,2),
    min_df=2,
    strip_accents='unicode'
)

tfidf_matrix = tfidf.fit_transform(books_for_model['text'])

tfidf_matrix.shape

# %% [markdown]
# ## Part 2: Build the Recommendation System

# %% [markdown]
# #### Build the Content-Based Recommender

# %%
#Creating an easy way to find ISBN to row index

isbn_to_index = pd.Series(books_for_model.index, index=books_for_model['ISBN'])

# %%
#Inspect top TF-IDF term

feature_names = tfidf.get_feature_names_out()

term_weights = tfidf_matrix.sum(axis=0).A1

N = 20 
top_idx = term_weights.argsort()[::-1][:N]

top_terms = [(feature_names[i], term_weights[i]) for i in top_idx]

top_terms

# %%
#Recommender function 

def recommend_content(isbn, top_n=10):
    idx = isbn_to_index [isbn]
    
    book_vec = tfidf_matrix[idx]
    
    sims = cosine_similarity(book_vec, tfidf_matrix).flatten()
    
    similar_idx = sims.argsort()[::-1][1:top_n+1]
    
    results = books_for_model.iloc[similar_idx][['ISBN', 'Book_Title', 'Book_Author', 'Publisher']].copy()

    results['similarity'] = sims[similar_idx]
    
    return results.reset_index(drop=True)

# %%
#Test the recommender

sample_isbn = books_for_model['ISBN'].iloc[0]
recommend_content(sample_isbn)

# %% [markdown]
# #### Collaborative Filtering Model

# %%
#Using surprise library
#It was specifically built for CF recommender systems

reader = Reader(rating_scale=(1,10))

data = Dataset.load_from_df(
    ratings_filt[['User_ID', 'ISBN', 'Book_Rating']],
    reader
)

trainset = data.build_full_trainset()

# %%
#Train an item-based cosine KNN model

sim_options = {
    'name': 'cosine',
    'user_based': False
}

algo = KNNBasic(sim_options= sim_options)
trainset = data.build_full_trainset()
algo.fit(trainset)

# %%
#Create mapping of ISBN to surprise internal ID

surprise_isbns = {trainset.to_raw_iid(iid) for iid in trainset.all_items()}

isbn_to_inner = {
    raw: trainset.to_inner_iid(raw)
    for raw in surprise_isbns
}

# %%
#Recommendation function
#Given an ISBN -> recommend similar books -> based on user rating patterns

def recommend_cf(isbn, top_n =10):
    if isbn not in isbn_to_inner:
        return f"ISBN {isbn} has no user ratings"
    
    inner_id = isbn_to_inner[isbn]

    neighbors_inner = algo.get_neighbors(inner_id, k=top_n)

    neighbor_isbns = [trainset.to_raw_iid(n) for n in neighbors_inner]

    recs = books_indexed.loc[neighbor_isbns][['Book_Title', 'Book_Author', 'Publisher']].copy()
    recs = recs.reset_index().rename(columns={'index': 'ISBN'})
   
    return recs

# %%
#Test the CF Model

sample_isbn = ratings_filt['ISBN'].iloc[0]
recommend_cf(sample_isbn)

# %%
#Double checking CF model

sample = ratings_filt['ISBN'].sample(1).iloc[0]
recommend_cf(sample)

# %% [markdown]
# #### Build Hybrid Recommender
# Combination of:
# 
# Content-based similarity - TF-IDF + Author + publisher
# 
# Collaborative filtering similarity - Surprise item-item cosine

# %%
#Get content similarity for a single ISBN

def get_content_similarities(isbn):
    if isbn not in isbn_to_index.index:
        return None
    
    idx = isbn_to_index[isbn]

    book_vec = tfidf_matrix[idx]

    sims = cosine_similarity(book_vec, tfidf_matrix).flatten()
    return sims

# %%
#Get CF similarity vector for the same ISBN

def get_cf_similarities(isbn, top_k=50):
    if isbn not in isbn_to_inner:
        return None
    
    inner_id = isbn_to_inner[isbn]

    neighbors_inner = algo.get_neighbors(inner_id, k=top_k)

    sim_vector = np.zeros(len(books_for_model), dtype=float)

    for n in neighbors_inner:
        sim = algo.sim[inner_id, n]
        raw_isbn = trainset.to_raw_iid(n)
    
        if raw_isbn in isbn_to_index.index:
            idx = isbn_to_index[raw_isbn]
            sim_vector[idx] = sim

    if sim_vector.max() > sim_vector.min():
        sim_vector= (sim_vector -sim_vector.min() / (sim_vector.max() - sim_vector.min()))

    return sim_vector

# %%
#Hybrid Function
#Also removes books the reader already rated (they read the book)

def recommend_hybrid_user(isbn, user_id, top_n=10, alpha=0.6):
    content_sims = get_content_similarities(isbn)
    cf_sims = get_cf_similarities(isbn)
    
    content_norm = content_sims / (content_sims.max() + 1e-6)
    cf_norm = cf_sims / (cf_sims.max() + 1e-6)

    hybrid_sims = alpha * content_norm + (1- alpha) * cf_norm
    
    hybrid_sims[isbn_to_index[isbn]] =0 

    user_rated = ratings_filt.loc[ratings_filt['User_ID'] == user_id, 'ISBN']
    user_rated_idx = isbn_to_index[user_rated.dropna()].values
    hybrid_sims[user_rated_idx] = 0

    top_idx = hybrid_sims.argsort()[::-1][:top_n]

    results = books_for_model.iloc[top_idx][['ISBN', 'Book_Title', 'Book_Author', 'Publisher']].copy()
    results['hybrid_score'] = hybrid_sims[top_idx]

    return results.reset_index(drop=True)

# %%
#Funtion that displays recs nicely

def show_hybrid_recs_user(isbn, user_id, top_n=10, alpha=0.6):
    book = books_for_model.loc[books_for_model['ISBN'] ==isbn].iloc[0]

    print("Book Read:")
    print(f"Title: {book['Book_Title']}")
    print(f"Author: {book['Book_Author']}")

    r = ratings_filt[
        (ratings_filt['ISBN'] == isbn) &
        (ratings_filt['User_ID'] == user_id)
        ].iloc[0]['Book_Rating']
    
    print(f"User Rating: {r}\n")

    print("\nTop 10 Recommendations:")
    recs = recommend_hybrid_user(isbn, user_id, top_n=top_n, alpha=alpha)
    display(recs)

# %%
#Test the hybrid model

#Pick a book a user read and rated

row = ratings_filt.sample(1).iloc[0]
user_id = row['User_ID']
isbn = row['ISBN']
user_rating = row['Book_Rating']

show_hybrid_recs_user(isbn, user_id, top_n=10, alpha=0.6)

# %%
#Double check 

#Pick a book a user read and rated

row = ratings_filt.sample(1).iloc[0]
user_id = row['User_ID']
isbn = row['ISBN']
user_rating = row['Book_Rating']

show_hybrid_recs_user(isbn, user_id, top_n=10, alpha=0.6)
```

</details>